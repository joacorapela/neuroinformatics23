\documentclass[12pt]{article}

\usepackage[hypertexnames=false,colorlinks=true,breaklinks]{hyperref}
\usepackage{graphicx}
\usepackage[shortlabels]{enumitem}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[title]{appendix}
\usepackage[margin=1in]{geometry}

\newtheorem{claim}{Claim}
\newtheorem{lemma}{Lemma}

\def\fig_width{3.5in}
\title{Report worksheet 1}
\author{Joaqu\'{i}n Rapela}

\begin{document}

\maketitle

\section*{Exercise 1: t-test for non-Gaussian distributions}

Under the null hypothesis, p-values should follow a uniform distribution (see
proof in Apendix~\ref{sec:pValuesAreUniform}). Therefore, when sampling from a
normal distribution with zero mean, we should observe
0.05*\texttt{n\_repeats}=50 tests with pvalues in the range $[p,
p+0.05],\;\forall p\in[0, 0.95]$. In particular, when sampling from a Normal
distribution with zero mean, we should observe 50 tests with p\_value\textless
0.05.

\begin{enumerate}[(a)]

    \item Here we are sampling from a normal distribution with zero mean. Thus,
        all histogram bins of length 0.05 should have around 50 counts. And
        this is what Fig.~\ref{fig:ex1a} shows.

        For a probability of type I error less than 0.05, the null hypothesis
        is rejected 5\% of the time when the null hypothesis is true.

        \begin{figure}[H]
            \begin{center}
                \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/ex1_distributionNormal_popmean0.0000_mean0.0000_nSamples10000_withErrorBars.html}{\includegraphics[width=\fig_width]{../figures/ex1_distributionNormal_popmean0.0000_mean0.0000_nSamples10000_withErrorBars.png}}

                \caption{Exercise 1a. Histogram of p-values of 1.000 t-tests
                evaluating if the mean of 10.000 samples from a $\mathcal{N}(0,
                1)$ is equal to zero.
                Click on the figure to see its interactive version.
                The script to generate this figure appears
                \href{https://github.com/joacorapela/neuroinformatics23/blob/master/worksheets/ws1/mySolution/code/scripts/doEx1WithErrorBars.py}{here} and the
                parameters used for this script appear
                \href{https://github.com/joacorapela/neuroinformatics23/blob/master/worksheets/ws1/mySolution/code/scripts/doEx1aWithErrorBars.csh}{here}.}

                \label{fig:ex1a}

            \end{center}
        \end{figure}

    \item  The previous item, and all the following ones, study the probability
        Type I error of a hypothesis test. This is the probabiliy of rejecting
        the null hypothesis when this hypothesis is true.
        %
        This item studies the probability of Type II error, which is the
        probability of accepting the null hypothesis when this hypothesis is
        false.

        Fig.~\ref{fig:typeIIerror}a illustrates Type I
        and II errors. Here the null hypothesis is that the observed samples
        come from a standard Gaussian distribution (mean 0 and standard
        deviation 1). The probability density function of the sample mean
        under the null hypothesis is the blue trace in
        Fig.~\ref{fig:typeIIerror}a.
        %
        If we accept a probability of type I error of at most 5\%
        (i.e., take $\alpha=0.05$), a type I error will occur if the
        observed sample mean is to the right of the dashed vertical line. The
        probability of this error is the area of the region colored in blue in
        Fig.~\ref{fig:typeIIerror}a.

        To calculate the probability of type II error we need to specify the
        distribution of an alternative hypothesis. In
        Fig.~\ref{fig:typeIIerror}a the distribution of
        the observations under the alternative hypothesis is a Gaussian with
        mean 2.5 and standard deviation 1.0. If we take $\alpha=0.05$, a type
        II error will occur if the observed sample mean is to the left of the
        dashed vertical line. The probability of this error is the area of the
        region colored in red in
        Fig.~\ref{fig:typeIIerror}a.

        To reduce the probability of Type II error we can:

        \begin{enumerate}[1.]

            \item accept a larger probability of type I error (i.e., reduce
                $\alpha$).
                Fig.~\ref{fig:typeIIerror}b
                illustrates the reduction of type II error when accepting a
                probability of type I error of 10\%. Compare with
                Fig.~\ref{fig:typeIIerror}a.

            \item use an alternative hypothesis with a mean more different from
                that of the null hypothesis (i.e., increase the effect size).
                Fig~.\ref{fig:typeIIerror}c
                illustrates the reduction of type II error when samples under
                the alternative hypothesis have a mean of 3.5 and standard
                deviation of 1. Compare with
                Fig.~\ref{fig:typeIIerror}a.

            \item increase the sample size. If $n$ samples come from a Gaussian
                distribution with mean $\mu$ and standard deviation $\sigma$
                (i.e., $x_i\sim\mathcal{N}(x_i|\mu,\sigma^2)$), then the sample
                mean will follow another Gaussian distribution with mean $\mu$
                and standard deviation $\frac{\sigma}{\sqrt{n}}$. Therefore, as
                the sample size increases, the variability of the sample mean
                decreases, both under the null and alternative hypothesis. Then
                the Gaussian distributions under the null and alternative
                hypothesis will overlap less, which will decrease the
                probability of type II error.
                Fig.~\ref{fig:typeIIerror}d shows
                the distributions of the sample means under the same null
                and alternative hypothesis as in 
                Fig.~\ref{fig:typeIIerror}a, but
                with an increased sample size of 250.
        \end{enumerate}
    
        \begin{figure}[H]
            \begin{center}
                \begin{subfigure}{0.4\textwidth}
                    \centering
                    \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/two_gaussians_mean00.0000_mean12.5000_std10.0000_alpha0.0500_nSamples100.html}{\includegraphics[width=\textwidth]{../figures/two_gaussians_mean00.0000_mean12.5000_std10.0000_alpha0.0500_nSamples100.png}}
                    \caption{Baseline distributions of $\bar{x}$ under the null
                    (blue trace) and alternative (red trace) hypothesis.}
                    \label{fig:ex1b_intro_a}
                \end{subfigure}
                \hfill
                \begin{subfigure}{0.4\textwidth}
                    \centering
                    \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/two_gaussians_mean00.0000_mean12.5000_std10.0000_alpha0.1000_nSamples100.html}{\includegraphics[width=\textwidth]{../figures/two_gaussians_mean00.0000_mean12.5000_std10.0000_alpha0.1000_nSamples100.png}}
                    \caption{Decreasing the probability of type II error by
                    increasing the probability of type I error.}
                    \label{fig:ex1b_intro_b}
                \end{subfigure}
                \hfill
                \begin{subfigure}{0.4\textwidth}
                    \centering
                    \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/two_gaussians_mean00.0000_mean13.5000_std10.0000_alpha0.0500_nSamples100.html}{\includegraphics[width=\textwidth]{../figures/two_gaussians_mean00.0000_mean13.5000_std10.0000_alpha0.0500_nSamples100.png}}
                    \caption{Decreasing the probability of type II error by
                    increasing the effect size.}
                    \label{fig:ex1b_intro_c}
                \end{subfigure}
                \hfill
                \begin{subfigure}{0.4\textwidth}
                    \centering
                    \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/two_gaussians_mean00.0000_mean12.5000_std10.0000_alpha0.0500_nSamples250.html}{\includegraphics[width=\textwidth]{../figures/two_gaussians_mean00.0000_mean12.5000_std10.0000_alpha0.0500_nSamples250.png}}
                    \caption{Decreasing the probability of type II error by
                    increasing the sample size.}
                    \label{fig:ex1b_intro_d}
                \end{subfigure}

                \caption{Exercise 1b. Type II errors in hypothesis testing and
                ways to decrease it. Click on any of the figures to see its
                interactive version.  The script to generate these figures
                appears
                \href{https://github.com/joacorapela/neuroinformatics23/blob/master/worksheets/ws1/mySolution/code/scripts/doPlotTwoGaussians.py}{here}
                and the parameters used for this script appear
                \href{https://github.com/joacorapela/neuroinformatics23/blob/master/worksheets/ws1/mySolution/code/scripts/doPlotTwoGaussians.py}{here}.}

                \label{fig:typeIIerror}
            \end{center}
        \end{figure}

        Fig.~\ref{fig:ex1b_part1}a shows a histogram of p-values for 1000
        one-sample t-tests checking if the mean of a sample of size 1000 from
        the $\mathcal{N}(0.1, 1)$ distribution is different from zero.
        %
        For all 1000 tests the null hypothesis was rejected at the 0.05
        criterion.
        %
        Fig.~\ref{fig:ex1b_part1}b shows the pdf of the sample mean under
        the null (blue trace) and the alternative (red trace) hypothesis.  That
        the t-test rejected the null hypothesis for all samples is
        explained by the fact that the area of the pdf of the sample mean under
        the alternative hypothesis (red trace) to the left of the vertical
        dashed line (i.e., the probability of type II error) is almost zero.

        From the discussion above we see that, if we decrease the sample size,
        the Gaussian distributions under the null and alternative hypothesis
        should overlap more, which will increase the probability of a type II
        error. Fig.~\ref{fig:ex1b_part1}c is identical to
        Fig.~\ref{fig:ex1b_part1}a, but uses a reduced sample size of 500.
        We observe an increase in  the counts for bins corresponding to p-values
        greater than 0.05, which implies that the probability of type II error
        increased.  Fig.~\ref{fig:ex1b_part1}d shows the probability
        density functions of the sample means under the null and alternative
        hypothesis. These pdf's overlap substantially more than those in
        Fig.~\ref{fig:ex1b_part1}b, which indicates a larger probability of
        type II error, in agreement with Fig.~\ref{fig:ex1b_part1}c.

        \begin{figure}[H]
            \begin{center}
                \begin{subfigure}{0.4\textwidth}
                    \centering
                    \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/ex1_distributionNormal_popmean0.0000_mean0.1000_nSamples10000_withErrorBars.html}{\includegraphics[width=\textwidth]{../figures/ex1_distributionNormal_popmean0.0000_mean0.1000_nSamples10000_withErrorBars.png}}
                    \caption{Histogram of p-values of 1.000 t-tests evaluating if the mean of 10.000 samples from a $\mathcal{N}(0.1, 1)$ is equal to zero.  Click on the figure to see its interactive version.}
                    \label{fig:ex1b_part1_a}
                \end{subfigure}
                \hfill
                \begin{subfigure}{0.4\textwidth}
                    \centering
                    \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/two_gaussians_mean00.0000_mean10.1000_std1.0000_alpha0.0500_nSamples10000.html}{\includegraphics[width=\textwidth]{../figures/two_gaussians_mean00.0000_mean10.1000_std1.0000_alpha0.0500_nSamples10000.png}}
                    \caption{Distributions of samples means under the null
                    hypothesis that samples come from a $\mathcal{N}(0, 1)$
                    (blue trace) and under the alternative hypothesis that
                    samples come from a $\mathcal{N}(0.1, 1)$ (red trace), for
                    a sample size of 10000.}
                    \label{fig:ex1b_part1_b}
                \end{subfigure}
                \hfill
                \begin{subfigure}{0.4\textwidth}
                    \centering
                    \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/ex1_distributionNormal_popmean0.0000_mean0.1000_nSamples500_withErrorBars.html}{\includegraphics[width=\textwidth]{../figures/ex1_distributionNormal_popmean0.0000_mean0.1000_nSamples500_withErrorBars.png}}
                    \caption{Histogram of p-values of 1.000 t-tests evaluating if the mean of 500 samples from a $\mathcal{N}(0.1, 1)$ is equal to zero.  Click on the figure to see its interactive version.}
                    \label{fig:ex1b_part1_c}
                \end{subfigure}
                \hfill
                \begin{subfigure}{0.4\textwidth}
                    \centering
                    \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/two_gaussians_mean00.0000_mean10.1000_std1.0000_alpha0.0500_nSamples500.html}{\includegraphics[width=\textwidth]{../figures/two_gaussians_mean00.0000_mean10.1000_std1.0000_alpha0.0500_nSamples500.png}}
                    \caption{Distributions of samples means under the null
                    hypothesis that samples come from a $\mathcal{N}(0, 1)$
                    (blue trace) and under the alternative hypothesis that
                    samples come from a $\mathcal{N}(0.1, 1)$ (red trace), for
                    a sample size of 500.}
                    \label{fig:ex1b_part1_d}
                \end{subfigure}

                \caption{Exercise 1b. Histogram of p-values from 1000
                one-sample t-tests used with samples from a $\mathcal{N}(0.1,
                1)$, and pdfs of the sample mean under  the null and
                alternative hypothesis.  The script to generate this figure
                appears
                \href{https://github.com/joacorapela/neuroinformatics23/blob/master/worksheets/ws1/mySolution/code/scripts/doEx1WithErrorBars.py}{here}
                and the parameters used for this script appear
                \href{https://github.com/joacorapela/neuroinformatics23/blob/master/worksheets/ws1/mySolution/code/scripts/doEx1bWithErrorBars.csh}{here}.}

                \label{fig:ex1b_part1}
            \end{center}
        \end{figure}

        The second part of this exercise asks to run t-tests using samples with
        a mean that is an order or magnitude more similar to the mean under the
        null hypothesis than the mean of the samples in the first part of this
        exercise. That is, in this second part we want to study the probability
        of type II error when the difference between the true and alternative
        hypothesis (i.e., the effect size) is smaller. As illustrated in
        Figs.~\ref{fig:typeIIerror}a and~\ref{fig:typeIIerror}c, decreasing the
        effect size should increase the probability of type II error. This
        increase is reflected in the larger number of counts for bins with
        p-values larger than 0.05 in Fig.~\ref{fig:ex1b_part2}a than in
        Fig.~\ref{fig:ex1b_part1}a, and by the larger overlap between the pdfs
        of the sample means under the null and alternative hypothesis in
        Fig.~\ref{fig:ex1b_part2}b than in Fig.~\ref{fig:ex1b_part1}b. 

        As illustrated in Fig.~\ref{fig:typeIIerror}d, increasing the sample
        size reduces the probability of type II error. In
        Figs.~\ref{fig:ex1b_part2}c and~d we increased the sample size from
        10000 to 50000.  Fig~.\ref{fig:ex1b_part2} shows that the overlap
        between the pdf of the sample mean under the null and alternative
        hypothesis decreased, and Fig.~\ref{fig:ex1b_part2}b shows that the
        proportion of type II errors decreased from 1-170/10000=83\% to
        1-610/1000=39\%.

        \begin{figure}[H]
            \begin{center}
                \begin{subfigure}{0.4\textwidth}
                    \centering
                    \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/ex1_distributionNormal_popmean0.0000_mean0.0100_nSamples10000_withErrorBars.html}{\includegraphics[width=\fig_width]{../figures/ex1_distributionNormal_popmean0.0000_mean0.0100_nSamples10000_withErrorBars.png}}
                    \caption{Histogram of p-values of 1.000 t-tests evaluating if the mean
                    of 10.000 samples from a $\mathcal{N}(0.01, 1)$ is equal to zero.}
                    \label{fig:ex1b_part2_a}
                \end{subfigure}
                \hfill
                \begin{subfigure}{0.4\textwidth}
                    \centering
                    \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/two_gaussians_mean00.0000_mean10.0100_std1.0000_alpha0.0500_nSamples10000.html}{\includegraphics[width=\textwidth]{../figures/two_gaussians_mean00.0000_mean10.0100_std1.0000_alpha0.0500_nSamples10000.png}}
                    \caption{Distributions of samples means under the null
                    hypothesis that samples come from a $\mathcal{N}(0, 1)$
                    (blue trace) and under the alternative hypothesis that
                    samples come from a $\mathcal{N}(0.01, 1)$ (red trace), for
                    a sample size of 10000.}
                    \label{fig:ex1b_part2_b}
                \end{subfigure}
                \begin{subfigure}{0.4\textwidth}
                    \centering
                    \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/ex1_distributionNormal_popmean0.0000_mean0.0100_nSamples50000_withErrorBars.html}{\includegraphics[width=\fig_width]{../figures/ex1_distributionNormal_popmean0.0000_mean0.0100_nSamples50000_withErrorBars.png}}
                    \caption{Histogram of p-values of 1.000 t-tests evaluating if the mean
                    of 50,000 samples from a $\mathcal{N}(0.01, 1)$ is equal to zero.}
                    \label{fig:ex1b_part2_c}
                \end{subfigure}
                \hfill
                \begin{subfigure}{0.4\textwidth}
                    \centering
                    \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/two_gaussians_mean00.0000_mean10.0100_std1.0000_alpha0.0500_nSamples50000.html}{\includegraphics[width=\textwidth]{../figures/two_gaussians_mean00.0000_mean10.0100_std1.0000_alpha0.0500_nSamples50000.png}}
                    \caption{Distributions of samples means under the null
                    hypothesis that samples come from a $\mathcal{N}(0, 1)$
                    (blue trace) and under the alternative hypothesis that
                    samples come from a $\mathcal{N}(0.01, 1)$ (red trace), for
                    a sample size of 50000.}
                    \label{fig:ex1b_part2_d}
                \end{subfigure}
                \caption{Exercise 1b.
                The script to generate this figure appears
                \href{https://github.com/joacorapela/neuroinformatics23/blob/master/worksheets/ws1/mySolution/code/scripts/doEx1WithErrorBars.py}{here} and the
                parameters used for this script appear
                \href{https://github.com/joacorapela/neuroinformatics23/blob/master/worksheets/ws1/mySolution/code/scripts/doEx1bWithErrorBars.csh}{here}.}
                \label{fig:ex1b_part2}
            \end{center}
        \end{figure}

    \item  Please refer to Fig.~\ref{fig:ex1c}.

        \begin{figure}[H]
            \begin{center}

                \begin{subfigure}{1.0\textwidth}
                    \centering
                    \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/ex1_distributionStdCauchy_popmean0.0000_mean0.0000_nSamples10000_withErrorBars.html}{\includegraphics[width=\fig_width]{../figures/ex1_distributionStdCauchy_popmean0.0000_mean0.0000_nSamples10000_withErrorBars.png}}

                    \caption{Histogram of p-values of 1.000 t-tests evaluating
                    if the mean of 10.000 samples from a standard Cauchy
                    distribution is equal to zero.  Click on the figure to see
                    its interactive version.}

                    \label{fig:ex1c_1}
                \end{subfigure}

                \begin{subfigure}{1.0\textwidth}
                    \centering
                    \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/ex1_distributionStdCauchy_popmean0.0000_mean0.0000_nSamples3_withErrorBars.html}{\includegraphics[width=\fig_width]{../figures/ex1_distributionStdCauchy_popmean0.0000_mean0.0000_nSamples3_withErrorBars.png}}

                    \caption{Histogram of p-values of 1.000 t-tests evaluating if the mean
                    of 3 samples from a standard Cauchy distribution is equal to zero.
                    Click on the figure to see its interactive version.}

                    \label{fig:ex1c_2}
                \end{subfigure}

                \caption{Exercise 1c.
                The script to generate this figure appears
                \href{https://github.com/joacorapela/neuroinformatics23/blob/master/worksheets/ws1/mySolution/code/scripts/doEx1WithErrorBars.py}{here} and the
                parameters used for this script appear
                \href{https://github.com/joacorapela/neuroinformatics23/blob/master/worksheets/ws1/mySolution/code/scripts/doEx1cWithErrorBars.csh}{here}.}
                \label{fig:ex1c}

            \end{center}
        \end{figure}

    \item  Please refer to Fig.~\ref{fig:ex1d}.

        \begin{figure}[H]
            \begin{center}

                \begin{subfigure}{1.0\textwidth}
                    \centering
                    \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/ex1_distributionRademacher_popmean0.0000_mean0.0000_nSamples10000_withErrorBars.html}{\includegraphics[width=\fig_width]{../figures/ex1_distributionRademacher_popmean0.0000_mean0.0000_nSamples10000_withErrorBars.png}}

                    \caption{Histogram of p-values of 1.000 t-tests evaluating
                    if the mean of 10.000 samples from a Rademacher
                    distribution is equal to zero.  Click on the figure to see
                    its interactive version.}

                    \label{fig:ex1d_1}
                \end{subfigure}

                \begin{subfigure}{1.0\textwidth}
                    \centering
                    \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/ex1_distributionRademacher_popmean0.0000_mean0.0000_nSamples3_withErrorBars.html}{\includegraphics[width=\fig_width]{../figures/ex1_distributionRademacher_popmean0.0000_mean0.0000_nSamples3_withErrorBars.png}}

                    \caption{Histogram of p-values of 1.000 t-tests evaluating if the mean
                    of 3 samples from a Rademacher distribution is equal to zero.
                    Click on the figure to see its interactive version.}

                    \label{fig:ex1d_2}
                \end{subfigure}

                \caption{Exercise 1d.
                The script to generate this figure appears
                \href{https://github.com/joacorapela/neuroinformatics23/blob/master/worksheets/ws1/mySolution/code/scripts/doEx1WithErrorBars.py}{here} and the
                parameters used for this script appear
                \href{https://github.com/joacorapela/neuroinformatics23/blob/master/worksheets/ws1/mySolution/code/scripts/doEx1dWithErrorBars.csh}{here}.}
                \label{fig:ex1d}

            \end{center}
        \end{figure}

    \item  Please refer to Fig.~\ref{fig:ex1e}.

        \begin{figure}[H]
            \begin{center}

                \begin{subfigure}{1.0\textwidth}
                    \centering
                    \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/ex1_distributionVerySkewed_popmean0.0010_mean0.0000_nSamples100_withErrorBars.html}{\includegraphics[width=\fig_width]{../figures/ex1_distributionVerySkewed_popmean0.0010_mean0.0000_nSamples100_withErrorBars.png}}

                    \caption{Histogram of p-values of 1.000 t-tests evaluating
                    if the mean of 100 samples from the very skewed
                    distribution distribution with mean 0.001 is equal to
                    0.001.  Click on the figure to see its interactive
                    version.}

                    \label{fig:ex1e_1}
                \end{subfigure}

                \begin{subfigure}{1.0\textwidth}
                    \centering
                    \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/ex1_distributionRademacher_popmean0.0000_mean0.0000_nSamples3_withErrorBars.html}{\includegraphics[width=\fig_width]{../figures/ex1_distributionRademacher_popmean0.0000_mean0.0000_nSamples3_withErrorBars.png}}

                    \caption{Histogram of p-values of 1.000 t-tests evaluating
                    if the mean of 100 samples from the very skewed
                    distribution distribution with mean 0.001 is equal to 0.0.
                    Click on the figure to see its interactive version.}

                    \label{fig:ex1e_2}
                \end{subfigure}

                \caption{Exercise 1e.
                The script to generate this figure appears
                \href{https://github.com/joacorapela/neuroinformatics23/blob/master/worksheets/ws1/mySolution/code/scripts/doEx1WithErrorBars.py}{here} and the
                parameters used for this script appear
                \href{https://github.com/joacorapela/neuroinformatics23/blob/master/worksheets/ws1/mySolution/code/scripts/doEx1eWithErrorBars.csh}{here}.}
                \label{fig:ex1e}

            \end{center}
        \end{figure}

\end{enumerate}

\section*{Exercise 2: randomization test}

Please refer to Fig.~\ref{fig:ex2}.

\begin{figure}[H]
    \begin{center}

        \begin{subfigure}{1.0\textwidth}
            \centering
            \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/ex2_distributionRademacher_popmean0.0000_mean0.0000_nSamples10.html}{\includegraphics[width=\fig_width]{../figures/ex2_distributionRademacher_popmean0.0000_mean0.0000_nSamples10.png}}

            \caption{Histogram of p-values of 1.000 randomization tests evaluating
            if the mean of 10 samples from the Rademacher
            distribution distribution is equal to 0.0.  Click on the
            figure to see its interactive version.}

            \label{fig:ex2_1}
        \end{subfigure}

        \begin{subfigure}{1.0\textwidth}
            \centering
            \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/ex2_distributionSkewed_popmean0.0000_mean0.0000_nSamples10.html}{\includegraphics[width=\fig_width]{../figures/ex2_distributionSkewed_popmean0.0000_mean0.0000_nSamples10.png}}

            \caption{Histogram of p-values of 1.000 randomization tests evaluating if the mean
            of 10 samples from the skewed distribution is equal to zero.
            Click on the figure to see its interactive version.}

            \label{fig:ex2_2}
        \end{subfigure}

        \caption{Exercise 2. The script to generate this figure appears
        \href{https://github.com/joacorapela/neuroinformatics23/blob/master/worksheets/ws1/mySolution/code/scripts/doEx2.py}{here}
        and the parameters used for this script appear
        \href{https://github.com/joacorapela/neuroinformatics23/blob/master/worksheets/ws1/mySolution/code/scripts/doEx2.csh}{here}.
        }

        \label{fig:ex2}

    \end{center}
\end{figure}

\section*{Exercise 3: raster plots}

Please refer to Fig.~\ref{fig:ex3}.

\begin{figure}[H]
    \begin{center}

        \begin{subfigure}{1.0\textwidth}
            \centering
            \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/spikes_times_clusterID41_epochedBystimOn_times_sortedByresponse_times_colorschoice.html}{\includegraphics[width=\fig_width]{../figures/spikes_times_clusterID41_epochedBystimOn_times_sortedByresponse_times_colorschoice.png}}

            \caption{Rasterplot of neuron 41 aligned to
            \texttt{stimOn\_times},
            sorted by
            \texttt{response\_times}
            and colored by
            % \texttt{choice}.
            }

            \label{fig:ex3_1}
        \end{subfigure}

        \begin{subfigure}{1.0\textwidth}
            \centering
            \href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ws1/figures/spikes_times_clusterID41_epochedBystimOn_times_sortedByresponse_times_colorsfeedbackType.html}{\includegraphics[width=\fig_width]{../figures/spikes_times_clusterID41_epochedBystimOn_times_sortedByresponse_times_colorsfeedbackType.png}}

            \caption{Rasterplot of neuron 41 aligned to
            \texttt{stimOn\_times},
            sorted by
            \texttt{response\_times}
            and colored by
            \texttt{feedbackType}.
            }

            \label{fig:ex3_2}
        \end{subfigure}

        \caption{Exercise 3. The script to generate this figure appears
        \href{https://github.com/joacorapela/neuroinformatics23/blob/master/worksheets/ws1/mySolution/code/scripts/doPlotEpochedSpikesTimes.py}{here}
        and the parameters used for this script appear
        \href{https://github.com/joacorapela/neuroinformatics23/blob/master/worksheets/ws1/mySolution/code/scripts/doPlotEpochedSpikesTimes.csh}{here}.
        }

        \label{fig:ex3}


    \end{center}
\end{figure}

\pagebreak
\begin{appendices}

\section{Under the null hypothesis, p-values are uniformly distributed in $[0,1]$}
\label{sec:pValuesAreUniform}

In Exercise~1, for $i=1,\ldots,1000$, we generated samples from random variables
$\{x_{(i,1)},\ldots,x_{(i,10000)}\}$, from these samples we computed a t-statistic
$t_i=f(x_{(i,1)},\ldots,x_{(i,10000)})$, and from this statistic we calculated a
p-value, $p_i=g(t_i)$. Because the t-statistic is a function, $f$, of random
variables, it can be considered as a random variable, $T$. Because the
p-value is a function, $g$, of a random variables, it can also be considered as
a random variable, $P$. The goal of this section is to prove that the p-value random
variable is uniformly distributed in [0,1]; i.e., $P\sim\mathcal{U}[0,1]$. This
proof is given in Lemma~\ref{lemma:p_values_uniform01}. Before giving this
proof we prove two auxiliary claims (Claims~\ref{claim:pvalue_as_function_of_stat}
and~\ref{claim:uniform_cummulative}).

Fig.~\ref{fig:pvalue} illustrates the concept of a p-value. It is the
probability of observing a statistic, $t$, greater than the observed one,
$t_\text{obs}$, when the null hipothesis is true.

\begin{figure}[H]
    \begin{center}
        % \includegraphics[width=\fig_width]{../figures/pvalue.png}

        \caption{Illustration of the p-value concept. A p-value is the
        probability of observing a statistic, $t$, greater than the observed
        one, $t_\text{obs}$, when the null hypothesis is true.}
        \label{fig:pvalue}

    \end{center}
\end{figure}

\begin{claim}

    Let $P$ be the p-values random variable, $T$ be the t-statistic random
    variable, and $F_T$ be the cummulative distribution function of $T$. Then
    $P=1-F_T(T)$
    \label{claim:pvalue_as_function_of_stat}

\end{claim}

\begin{proof}
    Let $t_{\text{obs},i}$ and $p_i$ be an observed statistic and associated
    p-value, respectively. Then,

    \begin{align}
        p_i=P(T>t_{\text{obs},i})=1-P(T<t_{\text{obs},i})=1-F_T(t_{\text{obs},i})\label{eq:pvalue_claim_tmp1}
    \end{align}

    The first equality in Eq.~\ref{eq:pvalue_claim_tmp1} follows from the
    definition of a p-value (Fig~\ref{fig:pvalue}). Because
    $p_i=1-F_T(t_{\text{obs},i})$ holds for any pair of samples $p_i$
    and $t_{\text{obs},i}$, then $P=1-F_T(T)$.
\end{proof}

\begin{claim}

    A random variable $U$ is uniformly distributed in [0,1]; i.e.,
    $U\sim\mathcal{U}[0,1]$, if and only if its cummulative distribution
    function is $F_U(u)=P(U<u)=u$, for $u\in[0,1]$.
    \label{claim:uniform_cummulative}

\end{claim}

\begin{proof}

    \begin{align}
        U\sim\mathcal{U}[0,1] &\iff f_U(u)=1 \text{ for } u\in[0,1] \text{ and } f_U(u)=0 \text{ elsewhere } \\
                              &\iff F_U(u)=P(U<u)=\int_0^pf_U(u)dp=u\text{, for }u\in[0,1].
    \end{align}

\end{proof}

\begin{lemma}

    When the null hypothesis holds, p-values are uniformly distributed in
    $[0,1]$; i.e., $P\sim\mathcal{U}[0,1]$.
    \label{lemma:p_values_uniform01}
\end{lemma}

\begin{proof}
    By Claim~\ref{claim:uniform_cummulative} it suffices to show that $F_P(p)=p$.

    \begin{align}
        F_P(p)&=P(P<p)=P(1-F_T(T)<p)=P(1-p<F_T(T)\label{eq:lemmaUniformPvaluesLine1}\\
              &=P(T>F_T^{-1}(1-1))=1-P(T<F_T^{-1}(1-p))\nonumber\\
              &=1-F_T(F_T^{-1}(1-p))=1-(1-p)=p\nonumber
    \end{align}

Note: the second equality in Eq.~\ref{eq:lemmaUniformPvaluesLine1} follows
from Claim~\ref{claim:pvalue_as_function_of_stat}.

\end{proof}

\end{appendices}

\end{document}
